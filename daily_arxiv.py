import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import datetime
import requests

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

base_url = "https://arxiv.paperswithcode.com/api/v0/papers/"
github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = ' OR ' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r', encoding="utf-8") as f:
        config = yaml.load(f,Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config

def get_authors(authors, first_author = False):
    output = str()
    if first_author == False:
        output = ", ".join(str(author) for author in authors)
    else:
        output = authors[0]
    return output
def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output
import requests

def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link

def get_daily_papers(topic,query="slam", max_results=2):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    # output
    content = dict()
    content_to_web = dict()
    content_struct = dict() 
    
    search_engine = arxiv.Search(
        query = query,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
    )

    for result in search_engine.results():

        paper_id            = result.get_short_id()
        paper_title         = result.title
        paper_url           = result.entry_id
        code_url            = base_url + paper_id #TODO
        paper_abstract      = result.summary.replace("\n"," ")
        paper_authors       = get_authors(result.authors)
        paper_first_author  = get_authors(result.authors,first_author = True)
        primary_category    = result.primary_category
        paper_categories    = result.categories
        publish_time        = result.published.date()
        update_time         = result.updated.date()
        comments            = result.comment

        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_first_author}")

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = arxiv_url + 'abs/' + paper_key

        try:
            # source code link
            # r = requests.get(code_url).json()
            repo_url = None
            # if "official" in r and r["official"]:
            #     repo_url = r["official"]["url"]
        except Exception as e:
            repo_url = None
            logging.error(f"exception: {e} with id: {paper_key}")

        try:
            # TODO: not found, two more chances
            # else:
            #    repo_url = get_code_link(paper_title)
            #    if repo_url is None:
            #        repo_url = get_code_link(paper_key)
            
            # Construct structural data
            paper_info = {
                "id": paper_key,
                "title": paper_title,
                "authors": paper_authors,
                "abstract": paper_abstract,
                "date": str(update_time),
                "pdf_url": paper_url,
                "code_url": repo_url if repo_url else "",
                "tags": paper_categories
            }
            content_struct[paper_key] = paper_info

            if repo_url is not None:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|**[link]({})**|\n".format(
                       update_time,paper_title,paper_first_author,paper_key,paper_url,repo_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({}), Code: **[{}]({})**".format(
                       update_time,paper_title,paper_first_author,paper_url,paper_url,repo_url,repo_url)

            else:
                content[paper_key] = "|**{}**|**{}**|{} et.al.|[{}]({})|null|\n".format(
                       update_time,paper_title,paper_first_author,paper_key,paper_url)
                content_to_web[paper_key] = "- {}, **{}**, {} et.al., Paper: [{}]({})".format(
                       update_time,paper_title,paper_first_author,paper_url,paper_url)

            # TODO: select useful comments
            comments = None
            if comments != None:
                content_to_web[paper_key] += f", {comments}\n"
            else:
                content_to_web[paper_key] += f"\n"

        except Exception as e:
            logging.error(f"exception: {e} with id: {paper_key}")

    data = {topic:content}
    data_web = {topic:content_to_web}
    data_struct = {topic:content_struct}

    return data, data_web, data_struct

def update_paper_links(filename):
    '''
    weekly update paper links in json file
    '''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        code = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date,title,authors,arxiv_id,code

    with open(filename,"r", encoding="utf-8") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

        json_data = m.copy()

        for keywords,v in json_data.items():
            logging.info(f'keywords = {keywords}')
            for paper_id,contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, code_url = parse_arxiv_string(contents)

                contents = "|{}|{}|{}|{}|{}|\n".format(update_time,paper_title,paper_first_author,paper_url,code_url)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')

                valid_link = False if '|null|' in contents else True
                if valid_link:
                    continue
                try:
                    # code_url = base_url + paper_id #TODO
                    # r = requests.get(code_url).json()
                    repo_url = None
                    # if "official" in r and r["official"]:
                    #     repo_url = r["official"]["url"]
                    #     if repo_url is not None:
                    #         new_cont = contents.replace('|null|',f'|**[link]({repo_url})**|')
                    #         logging.info(f'ID = {paper_id}, contents = {new_cont}')
                    #         json_data[keywords][paper_id] = str(new_cont)

                except Exception as e:
                    logging.error(f"exception: {e} with id: {paper_id}")
        # dump to json file
        with open(filename,"w", encoding="utf-8") as f:
            json.dump(json_data,f)

def update_json_file(filename,data_dict):
    '''
    daily update json file using data_dict
    '''
    if not os.path.exists(filename):
        with open(filename, "w", encoding="utf-8") as f:
            json.dump({}, f)
            
    with open(filename,"r", encoding="utf-8") as f:
        content = f.read()

        if not content:
            m = {}
        else:
            m = json.loads(content)

    json_data = m.copy()

    # update papers in each keywords
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename,"w", encoding="utf-8") as f:
        json.dump(json_data,f)

def export_to_html(json_filename, html_filename):
    """
    Export structural JSON data to an Apple-style Grid Layout HTML page with Sidebar and Search.
    """
    with open(json_filename, 'r', encoding='utf-8') as f:
        data = json.load(f)

    html_template = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CV Arxiv Daily</title>
    <style>
        :root {{
            --background-color: #f5f5f7;
            --card-background: #ffffff;
            --text-primary: #1d1d1f;
            --text-secondary: #86868b;
            --accent-color: #0071e3;
            --card-radius: 18px;
            --card-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --sidebar-width: 280px;
        }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text", "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            background-color: var(--background-color);
            color: var(--text-primary);
            margin: 0;
            padding: 0;
            line-height: 1.5;
            height: 100vh;
            display: flex;
            overflow: hidden;
        }}
        .app-container {{
            display: flex;
            width: 100%;
            height: 100%;
        }}
        
        /* Sidebar Styles */
        .sidebar {{
            width: var(--sidebar-width);
            background-color: #fbfbfd;
            border-right: 1px solid rgba(0,0,0,0.08); /* Faint divider */
            padding: 24px;
            display: flex;
            flex-direction: column;
            overflow-y: auto;
            flex-shrink: 0;
        }}
        .sidebar-header {{
            margin-bottom: 24px;
        }}
        .sidebar-header h1 {{
            font-size: 24px;
            font-weight: 700;
            margin: 0 0 4px 0;
            letter-spacing: -0.01em;
        }}
        .sidebar-header p {{
            font-size: 13px;
            color: var(--text-secondary);
            margin: 0;
        }}
        .search-container {{
            margin-bottom: 24px;
            position: relative;
        }}
        #searchInput {{
            width: 100%;
            padding: 10px 12px 10px 32px; /* Left padding for search icon if added */
            border-radius: 8px;
            border: 1px solid #d2d2d7;
            background-color: white;
            font-size: 14px;
            color: var(--text-primary);
            box-sizing: border-box;
            outline: none;
            transition: border-color 0.2s, box-shadow 0.2s;
        }}
        #searchInput:focus {{
            border-color: var(--accent-color);
            box-shadow: 0 0 0 3px rgba(0, 113, 227, 0.15);
        }}
        .nav-menu {{
            display: flex;
            flex-direction: column;
            gap: 4px;
        }}
        .nav-item {{
            display: block;
            padding: 8px 12px;
            border-radius: 8px;
            text-decoration: none;
            color: var(--text-primary);
            font-size: 14px;
            font-weight: 500;
            transition: background-color 0.2s;
        }}
        .nav-item:hover {{
            background-color: rgba(0,0,0,0.05);
        }}
        
        /* Main Content Styles */
        .main-content {{
            flex: 1;
            padding: 40px;
            overflow-y: auto;
            scroll-behavior: smooth;
        }}
        .section-container {{
            margin-bottom: 60px;
            scroll-margin-top: 40px; /* Offset for anchor links */
        }}
        .section-title {{
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 24px;
            padding-bottom: 12px;
            border-bottom: 1px solid #e5e5e5;
            color: var(--text-primary);
        }}
        
        /* Grid and Card Styles (Existing) */
        .grid {{
            display: flex;
            flex-direction: column;
            gap: 20px;
            max-width: 1000px;
            margin: 0 auto;
        }}
        .card {{
            background: var(--card-background);
            border-radius: var(--card-radius);
            box-shadow: var(--card-shadow);
            padding: 24px;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            display: flex;
            flex-direction: column;
            overflow: visible; 
            position: relative;
            z-index: 1;
        }}
        .card:hover {{
            transform: translateY(-2px);
            box-shadow: 0 8px 12px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            z-index: 10;
        }}
        .card-date {{
            font-size: 11px;
            color: var(--text-secondary);
            margin-bottom: 8px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            font-weight: 600;
        }}
        .card-title {{
            font-size: 18px;
            font-weight: 600;
            margin: 0 0 8px;
            line-height: 1.35;
            letter-spacing: -0.01em;
        }}
        .card-authors {{
            font-size: 13px;
            color: var(--text-secondary);
            margin-bottom: 16px;
            line-height: 1.4;
        }}        .tags-container {{
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 16px;
        }}
        .tag {{
            background-color: #f5f5f7;
            color: var(--text-secondary);
            font-size: 11px;
            padding: 4px 10px;
            border-radius: 100px;
            font-weight: 500;
        }}        .card-abstract-container {{
            position: relative;
            margin-bottom: 20px;
            flex-grow: 1;
        }}
        .card-abstract {{
            font-size: 14px;
            color: #424245;
            line-height: 1.6;
            text-align: justify;
        }}
        .card-index {{
            position: absolute;
            top: 24px;
            right: 24px;
            font-size: 24px;
            font-weight: 700;
            color: #e5e5e5;
            z-index: 0;
            pointer-events: none;
        }}
        .card-links {{
            display: flex;
            gap: 12px;
            margin-top: auto;
            position: relative;
            z-index: 2;
        }}
        .btn {{
            display: inline-block;
            padding: 6px 14px;
            border-radius: 100px;
            text-decoration: none;
            font-size: 12px;
            font-weight: 500;
            transition: background-color 0.2s;
        }}
        .btn-primary {{
            background-color: var(--accent-color);
            color: white;
        }}
        .btn-primary:hover {{
            background-color: #0077ed;
        }}
        .btn-secondary {{
            background-color: #f5f5f7;
            color: var(--text-primary);
        }}
        .btn-secondary:hover {{
            background-color: #e8e8ed;
        }}
        
        /* Empty State */
        .no-results {{
            text-align: center;
            padding: 40px;
            color: var(--text-secondary);
            font-size: 16px;
            display: none;
        }}
    </style>
</head>
<body>
    <div class="app-container">
        <!-- Sidebar -->
        <aside class="sidebar">
            <div class="sidebar-header">
                <h1>CV Arxiv</h1>
                <p style="font-size: 12px; color: var(--text-secondary);">Updated on {date_now}</p>
                {summary_text}
            </div>
            
            <div class="search-container">
                <input type="text" id="searchInput" placeholder="Search keywords, authors...">
            </div>
            
            <nav class="nav-menu">
                {nav_links}
            </nav>
        </aside>
        
        <!-- Main Content -->
        <main class="main-content">
            {content_sections}
            <div id="noResults" class="no-results">No papers found matching your search.</div>
        </main>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {{
            const searchInput = document.getElementById('searchInput');
            const cards = document.querySelectorAll('.card');
            const sections = document.querySelectorAll('.section-container');
            const navLinks = document.querySelectorAll('.nav-item');
            const noResults = document.getElementById('noResults');
            
            searchInput.addEventListener('input', function(e) {{
                const term = e.target.value.toLowerCase().trim();
                let hasVisibleCards = false;
                
                cards.forEach(card => {{
                    const title = card.querySelector('.card-title').textContent.toLowerCase();
                    const authors = card.querySelector('.card-authors').textContent.toLowerCase();
                    const abstract = card.querySelector('.card-abstract').textContent.toLowerCase();
                    
                    if (term === '' || title.includes(term) || authors.includes(term) || abstract.includes(term)) {{
                        card.style.display = 'flex';
                        hasVisibleCards = true;
                    }} else {{
                        card.style.display = 'none';
                    }}
                }});
                
                // Hide empty sections
                sections.forEach(section => {{
                    const visibleCards = section.querySelectorAll('.card[style="display: flex;"], .card:not([style*="display: none"])');
                    if (visibleCards.length === 0) {{
                        section.style.display = 'none';
                    }} else {{
                        section.style.display = 'block';
                    }}
                }});
                
                // Show/Hide no results message
                if (!hasVisibleCards && term !== '') {{
                    noResults.style.display = 'block';
                }} else {{
                    noResults.style.display = 'none';
                }}
            }});
        }});
    </script>
</body>
</html>
    """

    content_sections = ""
    nav_links = ""

    # Sort topics based on keys to ensure consistent order
    sorted_topics = sorted(data.keys())

    total_papers = 0
    topic_counts = []
    
    # Calculate summary
    summary_items = []
    for topic in sorted_topics:
        count = len(data[topic])
        if count > 0:
            total_papers += count
            summary_items.append(f'<div style="display: flex; justify-content: space-between; margin-bottom: 4px;"><span>{topic}</span> <span style="color: var(--text-primary); font-weight: 500;">{count}</span></div>')
    
    summary_text = f"""
    <div style="margin-top: 16px; font-size: 13px; color: var(--text-secondary);">
        <div style="font-weight: 600; font-size: 14px; margin-bottom: 12px; color: var(--text-primary);">Today: {total_papers} papers</div>
        <div style="display: flex; flex-direction: column;">
            {''.join(summary_items)}
        </div>
    </div>
    """

    global_index = 1

    for topic in sorted_topics:
        papers = data[topic]
        if not papers:
            continue
            
        topic_id = topic.lower().replace(" ", "-") # Create simple ID
        
        # Build Navigation Link
        nav_links += f'<a href="#{topic_id}" class="nav-item">{topic}</a>'
        
        # Build Section
        content_sections += f'<div id="{topic_id}" class="section-container">'
        content_sections += f'<div class="section-title">{topic}</div>'
        content_sections += '<div class="grid">'
        
        # Sort papers by date (descending)
        sorted_keys = sorted(papers.keys(), key=lambda x: papers[x]['date'], reverse=True)
        
        for key in sorted_keys:
            paper = papers[key]
            # Use 'get' to safely access fields just in case
            title = paper.get('title', 'No Title')
            authors = paper.get('authors', 'No Authors')
            abstract = paper.get('abstract', 'No Abstract available.')
            date = paper.get('date', '')
            pdf_url = paper.get('pdf_url', '#')
            code_url = paper.get('code_url', '')
            
            tags = paper.get('tags', [])
            tags_html = ""
            if tags:
                tags_html = '<div class="tags-container">'
                for tag in tags:
                     tags_html += f'<span class="tag">{tag}</span>'
                tags_html += '</div>'

            code_btn = ""
            if code_url:
                code_btn = f'<a href="{code_url}" class="btn btn-secondary" target="_blank">Code</a>'

            content_sections += f"""
            <div class="card">
                <div class="card-index">{global_index}</div>
                <div class="card-date">{date}</div>
                <div class="card-title">{title}</div>
                <div class="card-authors">{authors}</div>
                {tags_html}
                <div class="card-abstract-container">
                    <div class="card-abstract">{abstract}</div>
                </div>
                <div class="card-links">
                    <a href="{pdf_url}" class="btn btn-primary" target="_blank">PDF</a>
                    {code_btn}
                </div>
            </div>
            """
            global_index += 1
        content_sections += '</div></div>' # Close grid and section-container

    final_html = html_template.format(
        date_now=datetime.date.today().strftime('%Y-%m-%d'),
        summary_text=summary_text,
        content_sections=content_sections,
        nav_links=nav_links
    )

    with open(html_filename, 'w', encoding='utf-8') as f:
        f.write(final_html)
    logging.info(f"HTML exported to {html_filename}")

def json_to_md(filename,md_filename,
               task = '',
               to_web = False,
               use_title = True,
               use_tc = True,
               show_badge = True,
               use_b2t = True):
    """
    @param filename: str
    @param md_filename: str
    @return None
    """
    def pretty_math(s:str) -> str:
        ret = ''
        match = re.search(r"\$.*\$", s)
        if match == None:
            return s
        math_start,math_end = match.span()
        space_trail = space_leading = ''
        if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_trail = ' '
        if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_leading = ' '
        ret += s[:math_start]
        ret += f'{space_trail}${match.group()[1:-1].strip()}${space_leading}'
        ret += s[math_end:]
        return ret

    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-','.')

    with open(filename,"r", encoding="utf-8") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # clean README.md if daily already exist else create it
    with open(md_filename,"w+", encoding="utf-8") as f:
        pass

    # write data into README.md
    with open(md_filename,"a+", encoding="utf-8") as f:

        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")

        # if show_badge == True:
        #     f.write(f"[![Contributors][contributors-shield]][contributors-url]\n")
        #     f.write(f"[![Forks][forks-shield]][forks-url]\n")
        #     f.write(f"[![Stargazers][stars-shield]][stars-url]\n")
        #     f.write(f"[![Issues][issues-shield]][issues-url]\n\n")

        if use_title == True:
            #f.write(("<p align="center"><h1 align="center"><br><ins>CV-ARXIV-DAILY"
            #         "</ins><br>Automatically Update CV Papers Daily</h1></p>\n"))
            f.write("## Updated on " + DateNow + "\n")
        else:
            f.write("> Updated on " + DateNow + "\n")

        # TODO: add usage
        # f.write("> Usage instructions: [here](./docs/README.md#usage)\n\n")

        #Add: table of contents
        if use_tc == True:
            f.write("<details>\n")
            f.write("  <summary>Table of Contents</summary>\n")
            f.write("  <ol>\n")
            for keyword in data.keys():
                day_content = data[keyword]
                if not day_content:
                    continue
                kw = keyword.replace(' ','-')
                f.write(f"    <li><a href=#{kw.lower()}>{keyword}</a></li>\n")
            f.write("  </ol>\n")
            f.write("</details>\n\n")

        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            # the head of each part
            f.write(f"## {keyword}\n\n")

            if use_title == True :
                if to_web == False:
                    f.write("|Publish Date|Title|Authors|PDF|Code|\n" + "|---|---|---|---|---|\n")
                else:
                    f.write("| Publish Date | Title | Authors | PDF | Code |\n")
                    f.write("|:---------|:-----------------------|:---------|:------|:------|\n")

            # sort papers by date
            day_content = sort_papers(day_content)

            for _,v in day_content.items():
                if v is not None:
                    f.write(pretty_math(v)) # make latex pretty

            f.write(f"\n")

            #Add: back to top
            if use_b2t:
                top_info = f"#Updated on {DateNow}"
                top_info = top_info.replace(' ','-').replace('.','')
                f.write(f"<p align=right>(<a href={top_info.lower()}>back to top</a>)</p>\n\n")

        # if show_badge == True:
        #     # we don't like long string, break it!
        #     f.write((f"[contributors-shield]: https://img.shields.io/github/"
        #              f"contributors/Vincentqyw/cv-arxiv-daily.svg?style=for-the-badge\n"))
        #     f.write((f"[contributors-url]: https://github.com/Vincentqyw/"
        #              f"cv-arxiv-daily/graphs/contributors\n"))
        #     f.write((f"[forks-shield]: https://img.shields.io/github/forks/Vincentqyw/"
        #              f"cv-arxiv-daily.svg?style=for-the-badge\n"))
        #     f.write((f"[forks-url]: https://github.com/Vincentqyw/"
        #              f"cv-arxiv-daily/network/members\n"))
        #     f.write((f"[stars-shield]: https://img.shields.io/github/stars/Vincentqyw/"
        #              f"cv-arxiv-daily.svg?style=for-the-badge\n"))
        #     f.write((f"[stars-url]: https://github.com/Vincentqyw/"
        #              f"cv-arxiv-daily/stargazers\n"))
        #     f.write((f"[issues-shield]: https://img.shields.io/github/issues/Vincentqyw/"
        #              f"cv-arxiv-daily.svg?style=for-the-badge\n"))
        #     f.write((f"[issues-url]: https://github.com/Vincentqyw/"
        #              f"cv-arxiv-daily/issues\n\n"))

    logging.info(f"{task} finished")

def demo(**config):
    # TODO: use config
    data_collector = []
    data_collector_web= []
    data_collector_struct = []

    keywords = config['kv']
    max_results = config['max_results']
    publish_readme = config['publish_readme']
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']
    show_badge = config['show_badge']

    b_update = config['update_paper_links']
    logging.info(f'Update Paper Link = {b_update}')
    if config['update_paper_links'] == False:
        logging.info(f"GET daily papers begin")
        for topic, keyword in keywords.items():
            logging.info(f"Keyword: {topic}")
            data, data_web, data_struct = get_daily_papers(topic, query = keyword,
                                            max_results = max_results)
            data_collector.append(data)
            data_collector_web.append(data_web)
            data_collector_struct.append(data_struct)
            print("\n")
        logging.info(f"GET daily papers end")

    # Save structural data and generate HTML
    json_struct_file = './docs/cv-arxiv-daily-struct.json'
    html_file = 'index.html' # Root directory index.html
    
    # Use update_json_file for struct data as well (it merges data)
    try:
        if not config.get('update_paper_links', False):
             update_json_file(json_struct_file, data_collector_struct)
        
        # Always regenerate HTML based on the latest struct JSON
        export_to_html(json_struct_file, html_file)
    except Exception as e:
        logging.error(f"Error generating HTML or struct JSON: {e}")

    # 1. update README.md file
    if publish_readme:
        json_file = config['json_readme_path']
        md_file   = config['md_readme_path']
        # update paper links
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            # update json data
            update_json_file(json_file,data_collector)
        # json data to markdown
        json_to_md(json_file,md_file, task ='Update Readme', \
            show_badge = show_badge)

    # 2. update docs/index.md file (to gitpage)
    if publish_gitpage:
        json_file = config['json_gitpage_path']
        md_file   = config['md_gitpage_path']
        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file,data_collector)
        json_to_md(json_file, md_file, task ='Update GitPage', \
            to_web = True, show_badge = show_badge, \
            use_tc=False, use_b2t=False)

    # 3. Update docs/wechat.md file
    if publish_wechat:
        json_file = config['json_wechat_path']
        md_file   = config['md_wechat_path']
        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector_web)
        json_to_md(json_file, md_file, task ='Update Wechat', \
            to_web=False, use_title= False, show_badge = show_badge)
        

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path',type=str, default='config.yaml',
                            help='configuration file path')
    parser.add_argument('--update_paper_links', default=False,
                        action="store_true",help='whether to update paper links etc.')
    args = parser.parse_args()
    config = load_config(args.config_path)
    config = {**config, 'update_paper_links':args.update_paper_links}
    demo(**config)
